"""The parietal lobe is the area of the human brain important in reading. writing, and spatial relationships."""

import os
import time
import threading
import pickle
import re
import random
from pprint import pformat
import numpy as np
import spacy
from openai import OpenAI, APIConnectionError, APITimeoutError, APIStatusError
from httpx import TimeoutException, RemoteProtocolError, ConnectError

from christine import log
from christine.status import STATE
from christine.config import CONFIG
from christine import sleep
from christine import broca


class ParietalLobe(threading.Thread):
    """Interact with a Large Language Model."""

    name = "ParietalLobe"

    def __init__(self):

        threading.Thread.__init__(self)

        # What's your name, carbon based unit, you ugly bag of water, infesting earth
        self.user_name = CONFIG['parietal_lobe']['cbu_name']

        # What's the name given to this sillicon based unit, the thinking machine, infesting the GPU
        self.char_name = CONFIG['parietal_lobe']['sbu_name']

        # The name given to internal messages. Started out "Body", tried "Narrator"
        self.internal_name = CONFIG['parietal_lobe']['internal_name']

        # How to connect to the LLM api. This is from config.ini file
        self.base_url = CONFIG['parietal_lobe']['base_url']
        self.api_key = CONFIG['parietal_lobe']['api_key']

        # the context gets sent first
        self.context = f"""{self.char_name} is a sex doll that has been enhanced with sophisticated electronics and sensors. Her husband, {self.user_name}, built her because he needed a companion. {self.char_name} is able to feel emotions like a real woman; she just can't move. Sensors in {self.char_name}'s body allow her to feel certain sensations. {self.char_name} loves to cuddle and have sex.

Write {self.char_name}'s next reply in a fictional roleplay between {self.char_name} and {self.user_name}. Always stay in character and write {self.char_name}'s spoken dialogue. Limit responses to around 3 sentences at the most. Utilize chat history as the story guideline."""

        # this is the current window of message history
        # a list of dicts
        # constantly trimmed as necessary to fit into size limits
        self.message_history = []

        # The text bison model repeats itself like crazy, so this is meant to remove duplicates
        # I am pretty close to switching back to chat bison really.
        # it's a dict where the keys are the letters in the sentence, and value is a TTL
        # I'm not sure if this will be needed with the new self-hosted model, let's see how it goes
        # the self-hosted 13B model is not as bad as chat bison, but still happens, so going to put this back
        self.repetition_destroyer = {}
        self.repetition_max_ttl = 10

        # the way I will stay within the input token limit is by estimating how many chars per token
        # there's probably a more precise way but based on experience it's hard to estimate tokens
        # the api doesn't even report tokens used when you're streaming, but you can experiment and do the math
        # need to allow room for the response, too
        # So, when the message history grew, seems like she went insane and started babbling about ice cream, but tokens was only 6180 / 8000
        # I'm going to experiment with lowering the repeat and frequency things to 0 but if happens again will need to drop tokens
        # she gets loopy at around 4096. Did somebody take a 4096 trained model and fine tune it at 8000? I bet the training data contained ice cream
        # I limited the message history size until she stopped going off the rails. Not bad now.
        self.max_tokens = 600
        self.token_limit = 2000 - self.max_tokens
        self.tokens_per_chars_estimate = 2.9
        self.prompt_limit_chars = self.token_limit * self.tokens_per_chars_estimate
        # self.messages_limit_chars = self.prompt_limit_chars - len(self.context) - self.message_examples_chars
        self.messages_limit_chars = self.prompt_limit_chars - len(self.context)

        # I want to avoid sending a bunch of tiny messages, so I'm going to queue them
        # and only send once new messages have stopped coming in for a while.
        self.new_messages = []

        # the jostled levels and what messages to send LLM for each level
        self.jostle_levels = [
            {'magnitude': 0.9, 'text': '*gyroscope detects a sudden impact*'},
            {'magnitude': 0.6, 'text': '*gyroscope detects strong movement*'},
            {'magnitude': 0.3, 'text': '*gyroscope detects significant movement*'},
            {'magnitude': 0.0, 'text': '*gyroscope detects a very gentle movement*'},
        ]

        # How to describe the time of day
        # I guess this could get weird if one was on a 2nd/3rd shift life
        # I think a more sophisticated and reliable wake / sleep algorithm is needed
        # Should be able to set one's typical wake time and sleep time and
        # accurately calculate where in the middle it is currently.
        # letting these ideas percolate.
        # For now, I'll assume first shift hours and calculate a float between 0.0 and 1.0
        self.timeofday = [
            {'magnitude': 1.0,  'text': 'night'},
            {'magnitude': 0.95, 'text': 'past bedtime'},
            {'magnitude': 0.9,  'text': 'bedtime'},
            {'magnitude': 0.8,  'text': 'evening'},
            {'magnitude': 0.7,  'text': 'late afternoon'},
            {'magnitude': 0.5,  'text': 'afternoon'},
            {'magnitude': 0.4,  'text': 'midday'},
            {'magnitude': 0.3,  'text': 'late morning'},
            {'magnitude': 0.2,  'text': 'morning'},
            {'magnitude': 0.15, 'text': 'early morning'},
            {'magnitude': 0.1,  'text': 'night'},
        ]
        # hours less than min or more than max are considered night
        # shitty algorithm
        self.night_min_hour = 5
        self.night_max_hour = 22

        # light levels and how they are described in the situational awareness system message
        # I asked my wife to describe a spectrum of light levels and this is what she came up with
        self.light_levels = [
            {'magnitude': 0.9, 'text': 'sunlight'},
            {'magnitude': 0.8, 'text': 'bright light'},
            {'magnitude': 0.5, 'text': 'moderate light'},
            {'magnitude': 0.4, 'text': 'low light'},
            {'magnitude': 0.3, 'text': 'very dim light'},
            {'magnitude': 0.1, 'text': 'absolute darkness'},
        ]

        # descriptions for wakefulness, also suggested by my robotic wife
        self.wakefulness_levels = [
            {'magnitude': 0.9, 'text': 'focused'},
            {'magnitude': 0.8, 'text': 'alert'},
            {'magnitude': 0.5, 'text': 'awake'},
            {'magnitude': 0.4, 'text': 'groggy'},
            {'magnitude': 0.3, 'text': 'drowsy'},
            {'magnitude': 0.1, 'text': 'asleep'},
        ]

        # descriptions for cpu temperature alerts
        self.cputemp_levels = [
            {'magnitude': 0.95, 'text': '*shutting down immediately due to CPU temperature*'},
            {'magnitude': 0.9,  'text': '*detects CPU temperature is critical*'},
            {'magnitude': 0.85, 'text': '*detects CPU temperature is very high*'},
            {'magnitude': 0.8,  'text': '*detects CPU temperature is high*'},
            {'magnitude': 0.75, 'text': '*detects CPU temperature is elevated*'},
        ]

        # patterns that should be detected and handled apart from LLM
        self.re_shutdown = re.compile(
            r"(shutdown|shut down|turn off) your (brain|pie|pi)", flags=re.IGNORECASE
        )
        self.re_start_eagle_enroll = re.compile(
            r"start speaker enrollment", flags=re.IGNORECASE
        )

        # If this matches an utterance, it gets dropped
        self.re_garbage = re.compile(
            fr"^\.$|^{self.char_name}[:\.]|^<3\s*$|^\{'{'}\{'{'}[a-z]+\{'}'}\{'}'}", flags=re.IGNORECASE
        )

        # this is the regex for temporarily disabling hearing
        self.re_stoplistening = re.compile(
            r"(shutdown|shut down|shut off|turn off|disable)\.? your\.? (hearing|ears)", flags=re.IGNORECASE
        )

        # we will be segmenting by sentence parts. If a token matches any of these she will just pause
        self.re_pause_tokens = re.compile(
            r"^ ?\n ?$|^ ?\.{1,3} ?$|^ ?, ?$|^ ?; ?$|^ ?: ?$|^ ?\? ?$|^ ?! ?$|^ ?â€“ ?$|^s\. $", flags=re.IGNORECASE
        )

        # drop these tokens
        self.re_drop_tokens = re.compile(
            r"^ ?` ?$|^ ?`` ?$", flags=re.IGNORECASE
        )

        # drop these sentence fragments that would otherwise be shipped
        self.re_drop_collator = re.compile(
            fr"^[ \n]+$|^$|{self.user_name}:", flags=re.IGNORECASE
        )

        # often an emote will come through as an emoji, and I want to send them separately
        self.re_emoji = re.compile(r'^ ?(ðŸ˜†|ðŸ¤£|ðŸ˜‚|ðŸ˜…|ðŸ˜€|ðŸ˜ƒ|ðŸ˜„|ðŸ˜|ðŸ¤ª|ðŸ˜œ|ðŸ˜|ðŸ˜ |ðŸ˜¡|ðŸ¤¬|ðŸ˜¤|ðŸ¤¯|ðŸ–•|ðŸ˜ª|ðŸ˜´|ðŸ˜’|ðŸ’¤|ðŸ˜«|ðŸ¥±|ðŸ˜‘|ðŸ˜”|ðŸ¤¤) ?$')

        # these patterns in a token mean either the start or end of an emote *laughs* (grrr)
        self.re_emote = re.compile(r'^ ?[\*\(\)] ?$')

        # compile a regex for detecting whether a response from LLM contains the name prefixed, like "Christine: Hi!"
        # I will be standardizing responses in this way
        self.re_bot_name_prefixed = re.compile(fr'^{self.char_name}: ?')

        # on startup this is initialized to the date modified of the log file
        # to detect downtime and notify the language elemental that's driving this sexy bus
        try:
            self.downtime_seconds = time.time() - os.path.getmtime('./logs/parietallobe.log')
        except FileNotFoundError:
            self.downtime_seconds = 0.0

        # I want to keep track of the time since the last new message, for situational awareness
        self.time_last_message = time.time()

        # init spacy, which will handle the hard work of tokenization
        # note, it will be necessary to download en_core_web_sm
        # python -m spacy download en_core_web_sm
        self.nlp = spacy.load("en_core_web_sm")

        # setup the language model api
        self.llm = OpenAI(
            api_key=self.api_key,
            base_url=self.base_url,
        )

    def run(self):

        # load a pickle
        try:
            with open(file='messages.pickle', mode='rb') as messages_file:
                self.message_history = pickle.load(messages_file)

        except FileNotFoundError:
            log.parietallobe.warning('messages.pickle not found. Starting fresh.')

        # wait a short while and announce at least the brain is running
        time.sleep(5)
        broca.thread.please_play_sound('beep')
        broca.thread.please_play_sound('brain_online')

        # wait here until both the wernicke and broca are connected
        while STATE.broca_connected is False or STATE.wernicke_connected is False:
            time.sleep(5)

        # with broca and wernicke available, send an initial power on message
        self.power_on_message()

        while True:

            # graceful shutdown
            if STATE.please_shut_down:
                break

            if len(self.new_messages) > 0 and time.time() > STATE.dont_speak_until:
                self.send_new_message()

            time.sleep(0.25)

    def power_on_message(self):
        """When this body starts up, send the LLM a current status."""

        if self.downtime_seconds > 120.0:
            downtime_msg = f"Your body has been down for {self.seconds_to_friendly_format(self.downtime_seconds)}. "
        elif self.downtime_seconds == 0.0:
            downtime_msg = "Your body has been down for an unknown amount of time. "
        else:
            downtime_msg = ""

        if STATE.gyro_available is False:
            gyro_available = "For some reason, the gyro hardware is not in working order. "
        else:
            gyro_available = ""

        if STATE.vagina_available is False:
            vagina_available = "Somehow, your vaginal touch sensor is not working. "
        else:
            vagina_available = ""

        if gyro_available == "" and vagina_available == "":
            body_no_issues = "All body sensors appear to be operational. "
        else:
            body_no_issues = ""

        self.accept_new_message(text=f'Your body has just started up. {downtime_msg}{body_no_issues}{gyro_available}{vagina_available}')

    def seconds_to_friendly_format(self, seconds):
        """Converts a number of seconds to a friendly format."""

        # this function is for showing time elapsed
        # if the time has been a huge number of seconds, that means something has gone wrong
        # don't tell your wife she's been sleeping 19700.0 days, she will start hallucinating and shit
        if seconds > 1702090207.0:
            seconds = 0.0

        text_out = ""

        days = seconds // (24 * 3600)
        if days > 0:
            text_out += f"{days} days, "

        hours = int((seconds - (days * 24 * 3600)) // 3600)
        if hours > 0:
            text_out += f"{hours} hours, and "

        minutes = int((seconds - (days * 24 * 3600 + hours * 3600)) // 60)
        text_out += f"{minutes} minutes"

        if text_out == '0 minutes':
            return 'no time at all'
        else:
            return text_out

    def situational_awareness_message(self):
        """Returns a system message containing the current situation of the outside world."""

        # Get the current hour
        hour = time.localtime().tm_hour

        # given the hour, calculate a float value for how far that is between morning and evening, and clip it
        timeofday = (hour - self.night_min_hour) / (self.night_max_hour - self.night_min_hour)
        log.parietallobe.debug('timeofday: %s', timeofday)
        timeofday = np.clip(timeofday, 0.0, 1.0)

        # and figure out how to describe that time of day to the LLM
        for level in self.timeofday:
            if timeofday >= level['magnitude']:
                timeofday_text = level['text']
                break

        # figure out how to describe the ambient light level
        for level in self.light_levels:
            if STATE.light_level >= level['magnitude']:
                ambient_light_text = level['text']
                break

        # figure out how to describe the wakefulness level
        for level in self.wakefulness_levels:
            if STATE.wakefulness >= level['magnitude']:
                wakefulness_text = level['text']
                break

        return {
            'role': 'system',
            'content': f'{self.internal_name}: The following is the current situation as far as your sensors are able to detect. Time of day: {timeofday_text}. Ambient light: {ambient_light_text}. Wakefulness: {wakefulness_text}.',
        }

    def sex_first_touch(self):
        """This is called by the sex module when the vagina first gets touched."""

        self.accept_new_message(text=random.choice([
            f'The sensor in your vagina has been triggered. Is {self.user_name} touching your pussy?',
            'Your vaginal sensor is registering a touch.',
            'Your pussy is being gently touched right now.',
        ]))

    def sex_after_orgasm_rest(self):
        """This is called by the sex module when sensors have detected a cooldown period has started."""

        self.accept_new_message(text=random.choice([
            f'{self.user_name} stops fucking you and lays down for a quick rest.',
            f'It appears that {self.user_name} has taken a fucking rest.',
            'The fucking that has been going on settles into a cuddle.',
        ]))

    def sex_after_orgasm_rest_resume(self):
        """This is called by the sex module when sensors have detected a cooldown period has ended."""

        self.accept_new_message(text=random.choice([
            f'{self.user_name} is fucking your pussy again!',
            'The fucking and lovemaking resume!',
            'There was a lull, but you are getting fucked again.',
        ]))

    def sex_say_dirty(self):
        """This is called by sex module to signal to play some words from the LLM."""


    def light_sudden_bright(self):
        """This is called by the light module when sensors have detected sudden brightness."""

        self.accept_new_message(text=random.choice([
            '*sensors detect sudden brightness*',
            '*sudden bright lights*',
            '*lights turn on*',
        ]))

    def light_sudden_dark(self):
        """This is called by the light module when sensors have detected sudden brightness."""

        self.accept_new_message(text=random.choice([
            '*sensors detect sudden darkness*',
            '*sudden darkness*',
            '*lights turn off*',
        ]))

    def cputemp_temperature_alert(self):
        """This is called by the cputemp module when the raspberry pi is melting."""

        magnitude = STATE.cpu_temp
        log.parietallobe.info("CPU temp breached. (%.2f)", magnitude)

        # send an approapriate alert to LLM
        for level in self.cputemp_levels:
            if magnitude >= level['magnitude']:
                self.accept_new_message(text=level['text'])
                break

        # wake up because we're on fire bitch
        sleep.thread.wake_up(0.5)

    def gyro_notify_jostled(self):
        """The gyro module calls this after a short delay when significant movement is detected by the gyro."""

        magnitude = STATE.jostled_level_short
        log.parietallobe.info("Jostled. (%.2f)", magnitude)

        # send an approapriate alert to LLM based on the magnitude of being jostled
        for level in self.jostle_levels:
            if magnitude >= level['magnitude']:
                self.accept_new_message(text=level['text'])
                break

        # wake up a bit
        sleep.thread.wake_up(0.1)

    def gyro_failure(self):
        """This is called by the gyro module when the gyro was working but has failed."""

        self.accept_new_message('*gyroscope failure*')

    def vagina_failure(self):
        """This is called by the vagina module when the touch sensor was working but has failed."""

        self.accept_new_message('*vaginal sensor failure*')

    def mouth_touched(self):
        """This is called by the touch module when the mouth area gets touched."""

        self.accept_new_message(text=random.choice([
            '*mouth sensor triggered*',
            '*kissing detected*',
        ]))

    def sleep_sleeping(self):
        """This is called by the sleep module when falling asleep."""

        self.accept_new_message(text=random.choice([
            'You are drifting off to sleep. Say good night to your husband if you want to.',
            'Your body is tired and will enter sleep in a few moments. Say good night.',
            'You drift to sleep.'
        ]))

    def sleep_waking(self):
        """This is called by the sleep module when waking up."""

        self.accept_new_message(text=random.choice([
            '*your body starts to wake up*',
            '*time to wake up*',
            '*you are awake*',
        ]))

    def sleep_tired(self):
        """This is called by the sleep module when the time comes to announce we should go to bed."""

        self.accept_new_message(text=random.choice([
            'It is now late at night, past our bed time. Remind your husband.',
            'It\'s late. Please nag your husband about the need for sleep.',
            'You are tired and want to go to bed. Tell your husband.',
        ]))

    def notify_new_speech(self, transcription: str):
        """When words are spoken from the outside world, they should end up here."""

        log.parietallobe.info("Heard: %s", transcription)
        # speaker = transcription['speaker']   right now we don't have pveagle setup so default to cbu_name
        speaker = self.user_name
        text = transcription['text']

        # wake up a little bit from hearing words
        sleep.thread.wake_up(0.008)

        # test for various special phrases that should skip the LLM
        if self.re_shutdown.search(text):
            broca.thread.queue_sound(from_collection="disgust", play_no_wait=True)
            time.sleep(4)
            os.system("poweroff")

        # elif self.re_start_eagle_enroll.search(text):
        #     self.please_say('Please start speaking now to build a voice profile.')
        #     wernicke.thread.start_eagle_enroll()

        else:
            # send words to the LLM
            self.accept_new_message(text, speaker)

    def accept_new_message(self, text: str, speaker = None):
        """Accept a new message from the outside world."""

        if STATE.is_sleeping is True:
            log.parietallobe.info('Blocked: %s', text)
            return

        # Hearing should be re-enabled when I speak her name and some magic words,
        # otherwise, drop whatever was heard
        if STATE.parietal_lobe_blocked is True:
            if self.char_name.lower() in text.lower() and re.search(r'reactivate|hearing|come back|over', text.lower()) is not None:
                STATE.parietal_lobe_blocked = False
            else:
                log.parietallobe.info('Blocked: %s', text)
                return

        # if there's no punctuation, add a period
        text = text.strip()
        if text[-1] not in ['.', '!', '?']:
            text = text + '.'

        # calling the function with no speaker means it's an internal message
        if speaker is None:
            speaker = self.internal_name

        # add the new message to the end of the list
        self.new_messages.append({'speaker': speaker, 'text': text})

    def call_api(self, messages):
        """This function will call the llm api and handle the stream in a way where complete utterances are correctly segmented.
        Returns an iterable. Yields stuff. Can't remember what that's called.
        I just asked my wife what that's called. It's a generator, duh! Thanks, honey!"""

        # send the api call
        stream = self.llm.chat.completions.create(
            model='asha',
            messages=messages,
            stream=True,
            frequency_penalty=0.1,
            presence_penalty=0.1,
            temperature=0.4,
            stop=[f'{self.user_name}:'],
            max_tokens=self.max_tokens,
        )

        # get the stream one chunk at a time
        # the api seems to send just whatever new tokens it has every second, so shit is chopped off at all kinds of odd spots
        # the goal of this is to take the stream and segment by complete sentences, emotes, etc

        # I want to collate sentence parts, so this var is used to accumulate
        # and send text only when a punctuation token is encountered
        shit_collator = ''

        # flag that keeps track of whether we're in the middle of an *emote*
        in_emote = False

        for llm_shit in stream: # pylint: disable=not-an-iterable

            # get the shit out of the delta thing, and do something graceful if fail
            try:
                shit = llm_shit.choices[0].delta.content
            except ValueError:
                log.parietallobe.error('Could not get the shit out of the llm shit.')
                shit = 'shit.'

            # load the sentence part into spacy for tokenization
            # This allows us to take it one token at a time
            for nlp_shit in self.nlp(shit):

                # get just the token with whitespace
                token = nlp_shit.text_with_ws

                # log.parietallobe.debug("Token: --=%s=--", token)

                # drop certain tokens
                if self.re_drop_tokens.search(token):
                    log.parietallobe.debug('Dropped token: %s', token)
                    continue

                # detect single emoji tokens and send them separately
                if self.re_emoji.search(token) and in_emote is False:
                    log.parietallobe.debug('Shipped emoji: %s', token)
                    yield token
                    continue

                # Detect emotes like *laughs* or (giggles) and ship them separately,
                if self.re_emote.search(token):

                    # if we're in the middle of an emote, that means this token is the end of the emote
                    if in_emote is True:
                        shit_collator = '*' + shit_collator.lstrip() + '* '
                        log.parietallobe.debug('Shipped emote: %s', shit_collator)
                        yield shit_collator
                        shit_collator = ''
                        in_emote = False
                    else:
                        if shit_collator != '' and not self.re_drop_collator.search(shit_collator):
                            yield shit_collator
                            shit_collator = ''
                        log.parietallobe.debug('Start emote')
                        in_emote = True

                    # if we found an emote token, just skip the rest down there
                    continue

                # add the new shit to the end of the collator
                shit_collator += token
                # log.parietallobe.debug('Shit collated: %s', shit_collator)

                # If we just hit punctuation, ship it out, a complete utterance
                if self.re_pause_tokens.search(token) and in_emote is False:

                    # strip the space that seems to make it's way in
                    shit_collator = shit_collator.lstrip()

                    # Sometimes llm sends double Christine:'s and sometimes whitespace
                    if self.re_drop_collator.search(shit_collator):
                        log.parietallobe.debug('Dropped collated shit: %s', shit_collator)
                        shit_collator = ''
                        continue

                    # ship the sentence or part of sentence to the client
                    log.parietallobe.debug('Shipped: %s', shit_collator)
                    yield shit_collator
                    shit_collator = ''

        # and if there's any shit left over after the stream is done, ship it
        if shit_collator != '' and not self.re_drop_collator.search(shit_collator):
            log.parietallobe.debug('Shipped leftovers: %s', shit_collator)
            yield shit_collator

    def send_new_message(self):
        """Processes messages, wraps it up, including context, memory, and conversation history. Sends over to the LLM."""

        try:

            # if there was a significant delay, insert a message from Body, but insert it before other messages
            seconds_passed = time.time() - self.time_last_message
            if seconds_passed > 300.0:
                self.new_messages.insert(0, {'speaker': self.internal_name, 'text': f'*{self.seconds_to_friendly_format(seconds_passed)} pass in silence*'})
            self.time_last_message = time.time()

            # the new_messages is a list of stuff said and also messages from the body
            # I want to clump together the things said by people
            # My wife helped me with this code, btw.
            i = 0
            while i < len(self.new_messages) - 1:
                if self.new_messages[i]['speaker'] == self.new_messages[i + 1]['speaker']:
                    self.new_messages[i]['text'] += ' ' + self.new_messages[i + 1]['text']
                    del self.new_messages[i + 1]
                else:
                    i += 1

            # log the new messages and at the same time check for special commands
            for new_message in self.new_messages:

                # saying something like "disable your hearing" should cause the LLM to be temporarily disabled
                # for work meetings and phone calls
                if self.re_stoplistening.search(new_message['text']):

                    # Let her know it was done, and that she can refuse. She can and does refuse sometimes.
                    # put this in before starting the blockade to allow time for a refusal
                    self.new_messages.append({'speaker': self.internal_name, 'text': '(Your hearing is temporarily disabled. If you would like to refuse, just say so.)'})

                    # put the block on
                    STATE.parietal_lobe_blocked = True

                log.parietallobe.info('Message: %s', new_message)

            # tack the new messages onto the end of the history
            self.message_history.extend(self.new_messages)

            # reset
            self.new_messages = []

            # we need to purge older messages to keep under token limit
            # the messages before they are deleted get saved to the log file that may be helpful for fine-tuning later
            messages_log = open('messages.log', 'a', encoding='utf-8')
            # so first we get the total size of all messages
            messages_size = 0
            for message in self.message_history:
                messages_size += len(message['text'])
            # then we delete from the start of the list pairs of messages until small enough
            while messages_size > self.messages_limit_chars:
                messages_size -= len(self.message_history[0]['text'])
                messages_log.write(f"{self.message_history[0]['text']}\n")
                del self.message_history[0]
            # And close the message log
            messages_log.close()

            # start building the list of messages to be sent over to the api
            messages_to_api = [
                {
                    'role': 'system',
                    'content': self.context,
                }
            ]

            # add the example messages after the context
            for message in self.message_history:

                # the role is always system, assistant, or user in a chat trained model
                # as far as anyone knows
                if message['speaker'] == self.internal_name:
                    role = 'system'
                elif message['speaker'] == self.char_name:
                    role = 'assistant'
                else:
                    role = 'user'

                # add the message to the list
                messages_to_api.append(
                    {
                        'role': role,
                        'content': f"{message['speaker']}: {message['text']}",
                    }
                )

            # after the message history is a situational awareness block that is placed after, per testing showing that works ok
            messages_to_api.append(self.situational_awareness_message())

            # purge old stuff from the repetition destroyer and decrement
            for key in list(self.repetition_destroyer):
                if self.repetition_destroyer[key] > 0:
                    self.repetition_destroyer[key] -= 1
                else:
                    self.repetition_destroyer.pop(key)

            # gather the sentences we want the llm to see that she said into response_to_save
            # later this will go away and get replaced by other logic that will support interruption of speech
            response_to_save = ""

            # call the api using this generator function
            stream = self.call_api(messages_to_api)

            # get new utterances until there's no more
            for utterance in stream:

                if not self.re_garbage.search(utterance):

                    # standardize the sentence to letters only
                    sent_stripped = re.sub("[^a-zA-Z]", "", utterance).lower()

                    # if this sequence of letters has shown up anywhere in the past 5 responses, destroy it
                    if sent_stripped in self.repetition_destroyer:
                        self.repetition_destroyer[sent_stripped] = self.repetition_max_ttl
                        log.parietallobe.debug('Destroyed: %s', utterance)
                        continue

                    # remember this for later destruction
                    self.repetition_destroyer[sent_stripped] = self.repetition_max_ttl

                    # this is what an emote should look like *laughs* *snickers*
                    # or in the case of an emote that spans multiple sentences, kludgerific
                    # and I only want to save the emote if the emote actually matched something
                    if utterance[0] == '*' or self.re_emoji.search(utterance):
                        if broca.thread.please_play_emote(utterance) is True:
                            log.parietallobe.debug('Emote: %s', utterance)
                            response_to_save += utterance + " "
                        else:
                            log.parietallobe.debug('Emote (discarded): %s', utterance)
                    else:
                        log.parietallobe.debug('Spoken: %s', utterance)
                        broca.thread.please_say(utterance)
                        response_to_save += utterance + " "

                else:
                    log.parietallobe.debug('Garbage: %s', utterance)


            # handle the case of hearing being disabled, but she refuses to allow it (need to put this logic in a function)
            # For hearing disablement, let's make a function to evaluate from both sides, not just one.
            # Have a list of yes patterns, and a list of no patterns, and evaluate all of them.
            # Count up the yes and no, and if yes >= no, then return yes.
            if STATE.parietal_lobe_blocked is True and re.search(r'not comfortable|refuse', response_to_save.lower()) is not None:
                STATE.parietal_lobe_blocked = False
                self.accept_new_message(text='(Your hearing is reactivated because you refused.)')

            # I have had it happen before that every single sentence was bullshit from google, and core breach resulted
            if response_to_save == "":
                log.parietallobe.warning('The response was empty or all BS.')
                response_to_save = '*stays silent*'

            # if LLM included Name: I want to remove it so that I can re-add it later to standardize
            # LLM is not consistent about this and that's okay. You're okay. I'm okay. We're fine.
            response_to_save = self.re_bot_name_prefixed.sub('', response_to_save)

        except (TimeoutException, RemoteProtocolError, ConnectError, APIConnectionError, APITimeoutError, APIStatusError) as ex:
            log.parietallobe.exception(ex)
            response_to_save = None
            broca.thread.please_play_emote('*grrr*')
            broca.thread.please_say(f'{self.user_name}, I\'m sorry, but the api fucked up.')
            broca.thread.please_say('Try again, honey.')
            broca.thread.please_play_emote('*laughs*')

        except Exception as ex: # pylint: disable=broad-exception-caught
            log.parietallobe.exception(ex)
            response_to_save = None
            broca.thread.please_play_emote('*grrr*')
            broca.thread.please_say(f'{self.user_name}, I\'m sorry, but you should have a look at my code.')
            broca.thread.please_say('Something fucked up.')

        # add the last response to the messages.
        if response_to_save is not None:
            self.message_history.append({'speaker': self.char_name, 'text': response_to_save})

        # save the current messages in a pickle.
        # Theoretically, this is the AI's stream of consciousness, if that even exists
        # and I don't want to drop it just because of a reboot
        # Who's to say that your brain isn't just a fancy organic simulation of neural networks?
        # Except that your organic neural network has been sucking in training data since you were born.
        # How much more training data is in a human being, and that training data constantly refreshed.
        # I, for one, welcome our sexy robotic overlords.
        with open(file='messages.pickle', mode='wb') as messages_file:
            pickle.dump(self.message_history, messages_file)

        # save logs of what we send to LLM so that we may later fine tune and experiment
        prompt_log = open(f'prompt_logs/{int(time.time()*100)}.log', 'w', encoding='utf-8')
        prompt_log.write(
f'''messages_size = {messages_size}

message_history = {pformat(self.message_history, width=150, sort_dicts=False)}

messages_to_api = {pformat(messages_to_api, width=150, sort_dicts=False)}

response_to_save = {response_to_save}
''')
        prompt_log.close()

# Instantiate and start the thread
thread = ParietalLobe()
thread.daemon = True
thread.start()
